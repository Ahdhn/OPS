
// header
#define OPS_2D
#define OPS_ACC_MACROS
#define OPS_ACC_MD_MACROS
#include "ops_lib_cpp.h"
#ifdef OPS_MPI
#include "ops_mpi_core.h"
#endif
#include "user_types.h"

// global constants
extern double dx;
extern double dy;

//
// auto-generated by ops.py
//
#define OPS_ACC0(x, y)                                                         \
  (n_x * 1 + n_y * xdim0_poisson_kernel_stencil * 1 + x +                      \
   xdim0_poisson_kernel_stencil * (y))
#define OPS_ACC1(x, y)                                                         \
  (n_x * 1 + n_y * xdim1_poisson_kernel_stencil * 1 + x +                      \
   xdim1_poisson_kernel_stencil * (y))

// user function
extern "C" {
 void ops_par_loop_poisson_kernel_stencil_rec_execute(ops_kernel_descriptor *desc);

// host stub function
void ops_par_loop_poisson_kernel_stencil_rec_execute(ops_kernel_descriptor *desc) {
  ops_block block = desc->block;
  int dim = desc->dim;
  int *range = desc->range;
  ops_arg arg0 = desc->args[0];
  ops_arg arg1 = desc->args[1];

  // Timing
  double t1, t2, c1, c2;

  ops_arg args[2] = {arg0, arg1};

#ifdef CHECKPOINTING
  if (!ops_checkpointing_before(args, 2, range, 3))
    return;
#endif

  if (OPS_diags > 1) {
    OPS_kernels[3].count++;
    ops_timers_core(&c2, &t2);
  }

  // compute locally allocated range for the sub-block
  int start[2];
  int end[2];

  for (int n = 0; n < 2; n++) {
    start[n] = range[2 * n];
    end[n] = range[2 * n + 1];
  }

#ifdef OPS_DEBUG
  ops_register_args(args, "poisson_kernel_stencil");
#endif

  // set up initial pointers and exchange halos if necessary
  int base0 = args[0].dat->base_offset;
  const double *__restrict__ u = (double *)(args[0].data + base0);

  int base1 = args[1].dat->base_offset;
  double *__restrict__ u2 = (double *)(args[1].data + base1);

  // initialize global variable with the dimension of dats
  int xdim0_poisson_kernel_stencil = args[0].dat->size[0];
  int xdim1_poisson_kernel_stencil = args[1].dat->size[0];

  if (OPS_diags > 1) {
    ops_timers_core(&c1, &t1);
    OPS_kernels[3].mpi_time += t1 - t2;
  }

#ifndef start_1
  int start_1 = start[1];
  int start_0 = start[0];
  int end_1 = end[1];
  int end_0 = end[0];
#endif


#pragma omp parallel for
  for (int n_y = start_1; n_y < end_1; n_y++) {
#ifdef intel
#pragma loop_count(10000)
#pragma omp simd aligned(u, u2)
#else
#pragma simd
#endif
    for (int n_x = start_0; n_x < end_0; n_x++) {

      u2[OPS_ACC1(0, 0)] =
          ((u[OPS_ACC0(-1, 0)] - 2.0f * u[OPS_ACC0(0, 0)] + u[OPS_ACC0(1, 0)]) *
               0.125f +
           (u[OPS_ACC0(0, -1)] - 2.0f * u[OPS_ACC0(0, 0)] + u[OPS_ACC0(0, 1)]) *
               0.125f +
           u[OPS_ACC0(0, 0)]);
    }
  }
  if (OPS_diags > 1) {
    ops_timers_core(&c2, &t2);
    OPS_kernels[3].time += t2 - t1;
  }

  if (OPS_diags > 1) {
    // Update kernel record
    ops_timers_core(&c1, &t1);
    OPS_kernels[3].mpi_time += t1 - t2;
    OPS_kernels[3].transfer += ops_compute_transfer(dim, start, end, &arg0);
    OPS_kernels[3].transfer += ops_compute_transfer(dim, start, end, &arg1);
  }
}
}
#undef OPS_ACC0
#undef OPS_ACC1
